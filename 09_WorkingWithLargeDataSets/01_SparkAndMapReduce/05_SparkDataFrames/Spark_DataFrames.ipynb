{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Spark DataFrame: An Introduction\n",
    "The Spark DataFrame is a feature that allows you to create and work with DataFrame objects. As you may have guessed, pandas inspired it.\n",
    "\n",
    "Spark is well known for its ability to [process large data sets](https://opensource.com/business/15/1/apache-spark-new-world-record). Spark DataFrames combine the scale and speed of Spark with the familiar query, filter, and analysis capabilities of pandas. Unlike pandas, which can only run on one computer, Spark can use distributed memory (and disk when necessary) to handle larger data sets and run computations more quickly.\n",
    "\n",
    "Spark DataFrames allow us to modify and reuse our existing pandas code to scale up to much larger data sets. They also have better support for various data formats. We can even use a SQL interface to write distributed SQL queries that query large database systems and other data stores.\n",
    "\n",
    "For this mission, we'll be working with a JSON file containing data from the 2010 U.S. Census. It has the following columns:\n",
    "* age - Age (year)\n",
    "* females - Number of females\n",
    "* males - Number of males\n",
    "* total - Total number of individuals\n",
    "* year - Year column (2010 for all rows)\n",
    "\n",
    "Let's open and explore the data set before we dive into Spark DataFrames.\n",
    "\n",
    "#### Instructions\n",
    "Print the first four lines of __census_2010.json__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('census_2010.json')\n",
    "\n",
    "for i in range(0,4):\n",
    "    print(f.readline())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data\n",
    "In previous missions, we explored reading data into an RDD object. Recall that an RDD is essentially a list of tuples with no enforced schema or structure of any kind. An RDD can have a variable number of elements in each tuple, and combinations of types between tuples.\n",
    "\n",
    "RDDs are useful for representing unstructured data like text. Without them, we'd need to write a lot of custom Python code to interact with such data.\n",
    "\n",
    "We use the SparkContext object to read data into an RDD:\n",
    "\n",
    "    raw_data = sc.textFile(\\\"daily_show.tsv\\\")\n",
    "    daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "\n",
    "To use the familar DataFrame query interface from pandas, however, the data representation needs to include rows, columns, and types. Spark's implementation of DataFrames mirrors the pandas implementation, with logic for rows and columns.\n",
    "\n",
    "The Spark SQL class is very powerful. It gives Spark more information about the data structure we're using and the computations we want to perform. Spark uses that information to optimize processes.\n",
    "\n",
    "To take advantage of these features, we'll have to use the SQLContext object to structure external data as a DataFrame, instead of the SparkContext object.\n",
    "\n",
    "We can query Spark DataFrame objects with SQL, which we'll explore in the next mission. The SQLContext class gets its name from this capability.\n",
    "\n",
    "This class allows us to read in data and create new DataFrames from a wide range of sources. It can do this because it takes advantage of Spark's powerful [Data Sources API](https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html).\n",
    "\n",
    "File Formats:\n",
    "* JSON, CSV/TSV, XML\n",
    "* Parquet, Amazon S3 (cloud storage service)\n",
    "\n",
    "Big Data Systems\n",
    "* Hive\n",
    "* Avro\n",
    "* HBase\n",
    "\n",
    "SQL Database Systems\n",
    "* MySQL\n",
    "* PostgreSQL\n",
    "\n",
    "Data science organizations often use a wide range of systems to collect and store data, and they're constantly making changes to those systems. Spark DataFrames allow us to interface with different types of data, and ensure that our analysis logic will still work as the data storage mechanisms change.\n",
    "\n",
    "Now that you've learned a bit about Spark DataFrames, let's read in __census_2010.json__. This data set contains valid JSON on each line, which is what Spark needs in order to read the data in properly.\n",
    "\n",
    "In the following code cell, we:\n",
    "* Import SQLContext from the __pyspark.sql__ library\n",
    "* Instantiate the [SQLContext object](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.SQLContext) (which requires the SparkContext object (__sc__) as a parameter), and assign it to the variable __sqlCtx__\n",
    "* Use the SQLContext method __read.json()__ to read the JSON data set into a Spark DataFrame object named __df__\n",
    "* Print __df__'s data type to confirm that we successfully read it in as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to PySpark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import PySpark & initalize SparkContext object\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "# Import SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Pass in the SparkContext object `sc`\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "# Read JSON data into a DataFrame object `df`\n",
    "df = sqlCtx.read.json(\"census_2010.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "DataFrame[age: bigint, females: bigint, males: bigint, total: bigint, year: bigint]\n"
     ]
    }
   ],
   "source": [
    "print(type(df))\n",
    "print('')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "When we read data into the SQLContext object, Spark:\n",
    "* Instantiates a Spark DataFrame object\n",
    "* Infers the schema from the data and associates it with the DataFrame\n",
    "* Reads in the data and distributes it across clusters (if multiple clusters are available)\n",
    "* Returns the DataFrame object\n",
    "\n",
    "We expect the DataFrame Spark created to have the following columns, which were the keys in the JSON data set:\n",
    "* age\n",
    "* females\n",
    "* males\n",
    "* total\n",
    "* year\n",
    "\n",
    "Spark has its own type system that's similar to the pandas type system. To create a DataFrame, Spark iterates over the data set twice - once to extract the structure of the columns, and once to infer each column's type. Let's use one of the Spark DataFrame instance methods to display the schema for the DataFrame we're working with.\n",
    "\n",
    "#### Instructions\n",
    "* Call the __printSchema()__ [method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema) on the Spark DataFrame __df__ to display the schema that Spark inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- females: long (nullable = true)\n",
      " |-- males: long (nullable = true)\n",
      " |-- total: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas vs Spark DataFrames\n",
    "As we mentioned before, the pandas DataFrame heavily influenced the Spark DataFrame implementation. Here are some of the methods we can find in both:\n",
    "* agg()\n",
    "* join()\n",
    "* sort()\n",
    "* where()\n",
    "\n",
    "Unlike pandas DataFrames, however, Spark DataFrames are immutable, which means we can't modify existing objects. Most transformations on an object return a new DataFrame reflecting the changes instead. As we discussed in previous missions, Spark's creators deliberately designed immutability into Spark to make it easier to work with distributed data structures.\n",
    "\n",
    "Pandas and Spark DataFrames also have different underlying data structures. Pandas DataFrames are built around Series objects, while Spark DataFrames are built around RDDs. We can perform most of the same computations and transformations on Spark DataFrames that we can on pandas DataFrames, but the styles and methods are somewhat different. We'll explore how to perform common pandas functions with Spark in this mission.\n",
    "\n",
    "#### Instructions\n",
    "* Use the show() method to print the first five rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+----+\n",
      "|age|females|  males|  total|year|\n",
      "+---+-------+-------+-------+----+\n",
      "|  0|1994141|2085528|4079669|2010|\n",
      "|  1|1997991|2087350|4085341|2010|\n",
      "|  2|2000746|2088549|4089295|2010|\n",
      "|  3|2002756|2089465|4092221|2010|\n",
      "|  4|2004366|2090436|4094802|2010|\n",
      "+---+-------+-------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Objects\n",
    "In pandas, we used the __head()__ method to return the first __n__ rows. This is one of the differences between the DataFrame implementations. Instead of returning a nicely formatted table of values, the head() method in Spark returns a list of [row](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.Row) objects. Spark needs to return __row__ objects for certain methods, such as __head()__, __collect()__ and __take()__.\n",
    "\n",
    "You can access a row's attributes by the column name using dot notation, and by position using bracket notation with an index:\n",
    "\n",
    "    row_one = df.head(5)[0]\n",
    "    # Access value for age\n",
    "    row_one.age\n",
    "    # Access the first value\n",
    "    row_one[0]\n",
    "    \n",
    "#### Instructions\n",
    "* Use the __head()__ method to return the first five rows in the DataFrame as a list of __row__ objects, and assign the result to the variable __first_five__.\n",
    "* Print the __age__ value for each row object in first_five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=0, females=1994141, males=2085528, total=4079669, year=2010),\n",
       " Row(age=1, females=1997991, males=2087350, total=4085341, year=2010),\n",
       " Row(age=2, females=2000746, males=2088549, total=4089295, year=2010),\n",
       " Row(age=3, females=2002756, males=2089465, total=4092221, year=2010),\n",
       " Row(age=4, females=2004366, males=2090436, total=4094802, year=2010)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_five = df.head(5)\n",
    "first_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for r in first_five:\n",
    "    print(r.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Columns\n",
    "In pandas, we passed a string into a single pair of brackets __(\\[\\])__ to select an individual column, and passed in a list to select multiple columns:\n",
    "\n",
    "    # Pandas DataFrame\n",
    "    df['age']\n",
    "    df[['age', 'males']]\n",
    "\n",
    "We can still use bracket notation in Spark. We'll need to pass in a list of string objects, though, even when we're only selecting one column.\n",
    "\n",
    "Spark takes advantage of lazy loading with DataFrames, and will only display the results of an operation when we call the __show()__ method. Instead of using bracket notation, we can also use the __select()__ method to select columns:\n",
    "\n",
    "    # Spark DataFrame\n",
    "    df.select('age')\n",
    "    df.select('age', 'males')\n",
    "\n",
    "In the following code cell, we demonstrate how to select and display the __age__ column. Use what you've learned to take this a step farther and select multiple columns.\n",
    "\n",
    "#### Instructions\n",
    "* Select the __age__, __males__, and __females__ columns from the DataFrame and display them using the show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+\n",
      "|age|  males|females|\n",
      "+---+-------+-------+\n",
      "|  0|2085528|1994141|\n",
      "|  1|2087350|1997991|\n",
      "|  2|2088549|2000746|\n",
      "|  3|2089465|2002756|\n",
      "|  4|2090436|2004366|\n",
      "|  5|2091803|2005925|\n",
      "|  6|2093905|2007781|\n",
      "|  7|2097080|2010281|\n",
      "|  8|2101670|2013771|\n",
      "|  9|2108014|2018603|\n",
      "| 10|2114217|2023289|\n",
      "| 11|2118390|2026352|\n",
      "| 12|2132030|2037286|\n",
      "| 13|2159943|2060100|\n",
      "| 14|2195773|2089651|\n",
      "| 15|2229339|2117689|\n",
      "| 16|2263862|2146942|\n",
      "| 17|2285295|2165852|\n",
      "| 18|2285990|2168175|\n",
      "| 19|2272689|2159571|\n",
      "+---+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age','males','females').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same thing can be accomplished with the code below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+\n",
      "|age|  males|females|\n",
      "+---+-------+-------+\n",
      "|  0|2085528|1994141|\n",
      "|  1|2087350|1997991|\n",
      "|  2|2088549|2000746|\n",
      "|  3|2089465|2002756|\n",
      "|  4|2090436|2004366|\n",
      "|  5|2091803|2005925|\n",
      "|  6|2093905|2007781|\n",
      "|  7|2097080|2010281|\n",
      "|  8|2101670|2013771|\n",
      "|  9|2108014|2018603|\n",
      "| 10|2114217|2023289|\n",
      "| 11|2118390|2026352|\n",
      "| 12|2132030|2037286|\n",
      "| 13|2159943|2060100|\n",
      "| 14|2195773|2089651|\n",
      "| 15|2229339|2117689|\n",
      "| 16|2263862|2146942|\n",
      "| 17|2285295|2165852|\n",
      "| 18|2285990|2168175|\n",
      "| 19|2272689|2159571|\n",
      "+---+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['age', 'males', 'females']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Rows\n",
    "In pandas, we used Boolean filtering to select only the rows we were interested in. Spark preserves the very same functionality and notation.\n",
    "\n",
    "#### Instructions\n",
    "* Use the pandas notation for Boolean filtering to select the rows where __age__ is greater than five.\n",
    "* Assign the resulting DataFrame to the variable __five_plus__.\n",
    "* Use the __show()__ method to display __five_plus__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+----+\n",
      "|age|females|  males|  total|year|\n",
      "+---+-------+-------+-------+----+\n",
      "|  6|2007781|2093905|4101686|2010|\n",
      "|  7|2010281|2097080|4107361|2010|\n",
      "|  8|2013771|2101670|4115441|2010|\n",
      "|  9|2018603|2108014|4126617|2010|\n",
      "| 10|2023289|2114217|4137506|2010|\n",
      "| 11|2026352|2118390|4144742|2010|\n",
      "| 12|2037286|2132030|4169316|2010|\n",
      "| 13|2060100|2159943|4220043|2010|\n",
      "| 14|2089651|2195773|4285424|2010|\n",
      "| 15|2117689|2229339|4347028|2010|\n",
      "| 16|2146942|2263862|4410804|2010|\n",
      "| 17|2165852|2285295|4451147|2010|\n",
      "| 18|2168175|2285990|4454165|2010|\n",
      "| 19|2159571|2272689|4432260|2010|\n",
      "| 20|2151448|2259690|4411138|2010|\n",
      "| 21|2140926|2244039|4384965|2010|\n",
      "| 22|2133510|2229168|4362678|2010|\n",
      "| 23|2132897|2218195|4351092|2010|\n",
      "| 24|2135789|2208905|4344694|2010|\n",
      "| 25|2136497|2197148|4333645|2010|\n",
      "+---+-------+-------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "five_plus = df[df['age'] > 5]\n",
    "five_plus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Column Comparision as Filters\n",
    "We can compare the columns in Spark DataFrames with each other, and use the comparison criteria as a filter. For example, to get the rows where the population of males execeeded females in 2010, we'd write the same notation that we would use in pandas.\n",
    "\n",
    "#### Instructions\n",
    "* Find all of the rows where __females__ is less than __males__, and use __show()__ to display the first 20 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+----+\n",
      "|age|females|  males|  total|year|\n",
      "+---+-------+-------+-------+----+\n",
      "|  0|1994141|2085528|4079669|2010|\n",
      "|  1|1997991|2087350|4085341|2010|\n",
      "|  2|2000746|2088549|4089295|2010|\n",
      "|  3|2002756|2089465|4092221|2010|\n",
      "|  4|2004366|2090436|4094802|2010|\n",
      "|  5|2005925|2091803|4097728|2010|\n",
      "|  6|2007781|2093905|4101686|2010|\n",
      "|  7|2010281|2097080|4107361|2010|\n",
      "|  8|2013771|2101670|4115441|2010|\n",
      "|  9|2018603|2108014|4126617|2010|\n",
      "| 10|2023289|2114217|4137506|2010|\n",
      "| 11|2026352|2118390|4144742|2010|\n",
      "| 12|2037286|2132030|4169316|2010|\n",
      "| 13|2060100|2159943|4220043|2010|\n",
      "| 14|2089651|2195773|4285424|2010|\n",
      "| 15|2117689|2229339|4347028|2010|\n",
      "| 16|2146942|2263862|4410804|2010|\n",
      "| 17|2165852|2285295|4451147|2010|\n",
      "| 18|2168175|2285990|4454165|2010|\n",
      "| 19|2159571|2272689|4432260|2010|\n",
      "+---+-------+-------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df['females'] < df['males']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Spark DataFrames to pandas DataFrames\n",
    "The Spark DataFrame is fairly new, and the library's still a bit limited. There's no easy way to create a histogram of the data in a column, for example, or a line plot of the values in two columns.\n",
    "\n",
    "To handle some of these shortcomings, we can convert a Spark DataFrame to a pandas DataFrame using the __toPandas()__ method. Converting an entire Spark DataFrame to a pandas DataFrame works just fine for small data sets. For larger ones, though, we'll want to select a subset of the data that's more manageable for pandas.\n",
    "\n",
    "#### Instructions\n",
    "* Use the __toPandas()__ [method](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas) to convert the Spark DataFrame to a Pandas DataFrame, and assign it to the variable __pandas_df__.\n",
    "* Then, plot a histogram of the __total__ column using the __hist()__ [method](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.hist.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f966bac0410>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96832da350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas_df = df.toPandas()\n",
    "pandas_df['total'].hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
