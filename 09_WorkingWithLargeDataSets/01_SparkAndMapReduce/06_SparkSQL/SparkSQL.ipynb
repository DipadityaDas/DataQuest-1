{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In the previous mission, we learned how to read JSON into a Spark DataFrame, as well as some basic techniques for interacting with DataFrames. In this mission, we'll learn how to use Spark's SQL interface to query and interact with the data. We'll continue to work with the 2010 U.S. Census data set in this mission. Later on, we'll add other files to demonstrate how to take advantage of SQL to work with multiple data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the DataFrame as a Table\n",
    "Before we can write and run SQL queries, we need to tell Spark to treat the DataFrame as a SQL table. Spark internally maintains a virtual database within the SQLContext object. This object, which we enter as __sqlCtx__, has methods for registering temporary tables.\n",
    "\n",
    "To register a DataFrame as a table, call the __registerTempTable()__ [method](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable) on that DataFrame object. This method requires one string parameter, name, that we use to set the table __name__ for reference in our SQL queries.\n",
    "\n",
    "#### Instructions\n",
    "* Use the __registerTempTable()__ [method](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable) method to register the DataFrame __df__ as a table named __census2010__.\n",
    "* Then, run the SQLContext method __tableNames__ to return the list of tables.\n",
    "* Assign the resulting list to __tables__, and use the __print__ function to display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to PySpark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import PySpark & initalize SparkContext object\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "# Import SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Pass in the SparkContext object `sc`\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "# Read JSON data into a DataFrame object `df`\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "df.registerTempTable('census2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'census2010']\n"
     ]
    }
   ],
   "source": [
    "tables = sqlCtx.tableNames()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying\n",
    "Now that we've registered the table within __sqlCtx__, we can start writing and running SQL queries. With Spark SQL, we represent our query as a string and pass it into the __sql()__ [method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.sql) within the SQLContext object. The __sql()__ method requires a single parameter, the query string. Spark will return the query results as a spark DataFrame object. This means you'll have to use __show()__ to display the results, due to lazy loading.\n",
    "\n",
    "While SQLite requires that queries end with a semi-colon, Spark SQL will actually throw an error if you include it. Other than this difference in syntax, Spark's flavor of SQL is identical to SQLite, and all the queries you've written for the SQL course will work here as well.\n",
    "\n",
    "#### Instructions\n",
    "* Write a SQL query that returns the __age__ column from the table __census2010__ and the __show()__ method to display the first 20 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sqlCtx.sql(\"SELECT age FROM census2010 LIMIT 20\").show())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "In the previous mission, we used DataFrame methods to find all of the rows where __age__ was greater than 5. If we only wanted to retrieve data from the __males__ and __females__ columns where that criteria were true, we'd need to chain additional operations to the Spark DataFrame. To return the results in descending order instead of ascending order, we'd have to chain another method. The DataFrame methods are quick and powerful for simple queries, but chaining them can be cumbersome for more advanced queries.\n",
    "\n",
    "SQL shines at expressing complex logic in a more compact manner. Let's brush up on SQL by writing a query that expresses more specific criteria.\n",
    "\n",
    "#### Instructions\n",
    "Write and run a SQL query that returns:\n",
    "* The __males__ and __females__ columns (in that order) where __age__ > 5 and __age__ < 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|  males|females|\n",
      "+-------+-------+\n",
      "|2093905|2007781|\n",
      "|2097080|2010281|\n",
      "|2101670|2013771|\n",
      "|2108014|2018603|\n",
      "|2114217|2023289|\n",
      "|2118390|2026352|\n",
      "|2132030|2037286|\n",
      "|2159943|2060100|\n",
      "|2195773|2089651|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlCtx.sql('SELECT males, females FROM census2010 \\\n",
    "            WHERE age > 5 AND age < 15').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Functionality\n",
    "Because the results of SQL queries are DataFrame objects, we can combine the best aspects of both DataFrames and SQL to enhance our workflow. For example, we can write a SQL query that quickly returns a subset of our data as a DataFrame.\n",
    "\n",
    "#### Instructions\n",
    "* Write a SQL query that returns a DataFrame containing the __males__ and __females__ columns from the census2010 table.\n",
    "* Use the __describe()__ [method](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) to calculate summary statistics for the DataFrame and the __show()__ method to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|             males|          females|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|               101|              101|\n",
      "|   mean|1520095.3168316833|1571460.287128713|\n",
      "| stddev| 818587.2080168233|748671.0493484349|\n",
      "|    min|              4612|            25673|\n",
      "|    max|           2285990|          2331572|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlCtx.sql('SELECT males, females FROM census2010').describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
